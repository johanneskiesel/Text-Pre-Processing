{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing Techniques with spaCy and scikit-learn\n",
    "\n",
    "This notebook demonstrates essential text pre-processing techniques for Natural Language Processing (NLP) using Python. We use spaCy for linguistic analysis and scikit-learn for keyword extraction. Each section introduces a concept, explains its importance, and provides code to apply it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objective\n",
    "\n",
    "This tutorial will teach you the essential techniques for text pre-processing using Python and spaCy, with a focus on practical applications in social science research. You will learn how to clean, structure, and transform raw text dataâ€”making it ready for analysis, modeling, and interpretation.\n",
    "\n",
    "Text pre-processing is a critical first step in any Natural Language Processing (NLP) workflow. By mastering these methods, you will be able to:\n",
    "- Remove noise and inconsistencies from textual data\n",
    "- Standardize and normalize language for better analysis\n",
    "- Extract meaningful information for downstream tasks such as sentiment analysis, topic modeling, and entity recognition\n",
    "\n",
    "Whether you are working with survey responses, interview transcripts, or social media data, these skills will help you unlock deeper insights and make your research\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Science Use Cases for Text Pre-Processing\n",
    "\n",
    "Text pre-processing is a crucial step in social science research, enabling scholars to analyze large volumes of qualitative data efficiently and accurately. Here are some practical applications:\n",
    "\n",
    "- **Survey and Interview Analysis:**  \n",
    "  Automatically extract key themes, sentiments, and entities from open-ended survey responses or interview transcripts. For example, lemmatization and stopword removal help in identifying the most frequent topics discussed by participants.\n",
    "\n",
    "- **Political Discourse Analysis:**  \n",
    "  Tokenization, named entity recognition, and sentiment analysis can be used to study political speeches, debates, or social media posts. Researchers can track how politicians discuss certain issues, measure emotional tone, and identify key actors or organizations.\n",
    "\n",
    "- **Media and News Studies:**  \n",
    "  Use sentence segmentation and TF-IDF keyword extraction to compare coverage of events across different news outlets. Named entity recognition helps in mapping relationships between people, places, and organizations mentioned in articles.\n",
    "\n",
    "- **Comparative Linguistic Studies:**  \n",
    "  Vocabulary comparison functions allow researchers to analyze language differences between demographic groups, regions, or time periods. This is useful for studying language evolution, cultural trends, or the impact of policy changes.\n",
    "\n",
    "- **Public Opinion and Sentiment Tracking:**  \n",
    "  Sentiment analysis provides insights into public attitudes toward policies, social issues, or brands by analyzing social media, forums, or feedback forms.\n",
    "\n",
    "By applying these techniques, social scientists can transform unstructured text into actionable data, uncover hidden patterns, and support evidence-based decision-making in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Audience\n",
    "\n",
    "This project is designed for:\n",
    "\n",
    "- **Social Scientists and Researchers:**  \n",
    "  Who want to analyze qualitative data from surveys, interviews, or media sources using modern NLP techniques.\n",
    "\n",
    "- **Students and Educators:**  \n",
    "  Looking for a practical introduction to text pre-processing and its applications in social science research.\n",
    "\n",
    "- **Data Analysts and Practitioners:**  \n",
    "  Interested in cleaning, structuring, and extracting insights from large volumes of textual data.\n",
    "\n",
    "- **Anyone New to NLP:**  \n",
    "  The step-by-step notebook and clear code examples make it accessible for beginners with basic Python knowledge.\n",
    "\n",
    "No prior experience with spaCy or advanced machine learning is required. The tutorial guides you through each concept, making it easy to apply these techniques to your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration \n",
    "~ 45 mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's start by installing and importing the necessary libraries for text processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.8.7)\n",
      "Requirement already satisfied: scikit-learn in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: numpy in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 1)) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 1)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 1)) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 1)) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from jinja2->spacy->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: wrapt in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 1)) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/shyam/miniconda/envs/uni/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 1)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import os\n",
    "## we will use the pre_processing module for all text pre-processing tasks\n",
    "from pre_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading spaCy Language Models\n",
    "\n",
    "To process text in different languages, we need to load the appropriate spaCy model. The following functions help load and manage language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(language):\n",
    "    \"\"\"\n",
    "    Loads a spaCy language model for the specified language. If the model is not found,\n",
    "    attempts to download it and then load it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spacy.load(f\"{language}_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(f\"Model '{language}_core_web_sm' not found. Downloading...\")\n",
    "        try:\n",
    "            download_command = f\"python -m spacy download {language}_core_web_sm\"\n",
    "            exit_code = os.system(download_command)\n",
    "        except:\n",
    "            raise ValueError(f\"Language '{language}' is not supported.\")\n",
    "        return spacy.load(f\"{language}_core_web_sm\")\n",
    "\n",
    "def choose_spacy_model(language):\n",
    "    \"\"\"\n",
    "    Choose the appropriate spaCy language model based on the input language.\n",
    "    \"\"\"\n",
    "    return get_model(language)\n",
    "\n",
    "nlp = choose_spacy_model(\"en\")  # Default to English model, can be changed as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "**Tokenization** is the process of splitting text into individual words or tokens. This is the first step in most NLP pipelines.\n",
    "\n",
    "- Useful for: word frequency analysis, further linguistic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', 'with', 'most', 'accuracy', '.', 'it', 'also', 'allows', 'for', 'analyzing', 'text', 'data', 'more', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing enables computers to understand human language with most accuracy. it also allows for analyzing text data more effectively.\"\n",
    "\n",
    "tokens = tokenize_text(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The output displays each word and punctuation mark as a separate token. This allows us to analyze the structure and content of the text at the word level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing Stopwords\n",
    "\n",
    "**Stopwords** are common words (like \"the\", \"is\", \"and\") that usually do not add significant meaning to text analysis. Removing them helps focus on meaningful words.\n",
    "\n",
    "- Useful for: keyword extraction, topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stopwords: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'understand', 'human', 'language', 'accuracy', '.', 'allows', 'analyzing', 'text', 'data', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens without stopwords:\", remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The result contains only the meaningful words, with common stopwords removed. This helps focus analysis on the most relevant terms in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatization\n",
    "\n",
    "**Lemmatization** reduces words to their base or dictionary form (lemma). For example, \"running\" becomes \"run\".\n",
    "\n",
    "- Useful for: reducing vocabulary size, improving matching in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized tokens: ['Natural', 'Language', 'processing', 'enable', 'computer', 'to', 'understand', 'human', 'language', 'with', 'most', 'accuracy', '.', 'it', 'also', 'allow', 'for', 'analyze', 'text', 'datum', 'more', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = lemmatize_text(text)\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "Each word is reduced to its base form (lemma), which standardizes variations and improves the accuracy of further text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentence Segmentation\n",
    "\n",
    "**Sentence segmentation** splits text into individual sentences. This is useful for analyzing sentence structure and readability.\n",
    "\n",
    "- Useful for: readability analysis, sentiment per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Natural Language Processing enables computers to understand human language with most accuracy.', 'it also allows for analyzing text data more effectively.']\n"
     ]
    }
   ],
   "source": [
    "sentences = split_sentences(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The output lists each sentence found in the text. This segmentation allows us to analyze text structure, readability, and perform sentence-level operations such as sentiment analysis or topic detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Named Entity Recognition (NER)\n",
    "\n",
    "**Named Entity Recognition** identifies and classifies key entities in text, such as people, organizations, and locations.\n",
    "\n",
    "- Useful for: extracting actors, places, and organizations from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [{'text': 'los angeles', 'label': 'GPE', 'description': 'Countries, cities, states'}]\n"
     ]
    }
   ],
   "source": [
    "text_NER = 'the film was shot in los angeles and many other locations.'\n",
    "entities = extract_named_entities(text_NER)\n",
    "print(\"Named Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The output lists the named entities found in the text, such as people, organizations, and locations. This is useful for extracting key actors and places from documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Keyword Extraction with TF-IDF\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** highlights important words and phrases in a collection of documents.\n",
    "\n",
    "- Useful for: finding distinctive themes, comparing language use across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Keywords: [('learning', 0.22222222222222224), ('language', 0.20100756305184245), ('analytic', 0.13608276348795434), ('large', 0.13608276348795434), ('large volume', 0.13608276348795434), ('text', 0.13608276348795434), ('text analytic', 0.13608276348795434), ('volume', 0.13608276348795434), ('intelligence', 0.11111111111111112), ('learning deep', 0.11111111111111112), ('learning important', 0.11111111111111112), ('machine', 0.11111111111111112), ('machine learning', 0.11111111111111112), ('language processing', 0.10050378152592122), ('natural', 0.10050378152592122), ('natural language', 0.10050378152592122), ('processing', 0.10050378152592122), ('processing enable', 0.10050378152592122), ('understand', 0.10050378152592122), ('understand human', 0.10050378152592122)]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Natural Language Processing enables computers to understand human language.\",\n",
    "    \"Machine learning and deep learning are important for artificial intelligence.\",\n",
    "    \"Text analytics helps in extracting insights from large volumes of data.\"\n",
    "]\n",
    "\n",
    "keywords = extract_keywords_tfidf(texts)\n",
    "print(\"TF-IDF Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The result shows the most important keywords and phrases identified by TF-IDF across the provided documents. These keywords represent distinctive themes and help summarize the main topics present in the text collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Keyword Extraction: Lemmatized vs. Non-Lemmatized Text\n",
    "\n",
    "Keyword extraction using TF-IDF can yield different results depending on whether the input text is lemmatized. Lemmatization reduces words to their base forms, which helps group similar words and may improve the relevance of extracted keywords. Here, we compare the keywords extracted from raw text and lemmatized text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Keywords (Raw Text): [('and', 0.32883558900175286), ('language', 0.3110039837944526), ('data', 0.2880384333605476), ('text', 0.2880384333605476), ('learning', 0.24782896832835802), ('learning models', 0.17803112498119444), ('models', 0.17803112498119444), ('learning are', 0.1478341860014219), ('machine', 0.1478341860014219), ('machine learning', 0.1478341860014219)]\n",
      "TF-IDF Keywords (Lemmatized Text): [('language', 0.4124189628885659), ('learning', 0.2784754321986805), ('learning model', 0.226972853466228), ('datum', 0.22575685555911473), ('text', 0.22575685555911473), ('language processing', 0.1576542605386584), ('large', 0.1391888746260641), ('large volume', 0.1391888746260641), ('learning important', 0.1391888746260641), ('machine', 0.1391888746260641)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"Natural Language Processing enables computers to understand human languages and process text data efficiently.\",\n",
    "    \"Text analytics and machine learning are important for extracting insights from large volumes of textual data.\",\n",
    "    \"Deep learning models help in analyzing and comprehending complex language patterns.\"\n",
    "]\n",
    "\n",
    "# Keyword extraction on raw (non-lemmatized) texts\n",
    "vectorizer_raw = TfidfVectorizer(max_features=10, ngram_range=(1,2))\n",
    "tfidf_matrix_raw = vectorizer_raw.fit_transform(texts)\n",
    "keywords_raw = list(zip(vectorizer_raw.get_feature_names_out(), tfidf_matrix_raw.mean(axis=0).A1))\n",
    "keywords_raw.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"TF-IDF Keywords (Raw Text):\", keywords_raw)\n",
    "\n",
    "# Lemmatize texts using spaCy and remove stopwords/punctuation\n",
    "lemmatized_texts = []\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    lemmatized = ' '.join([token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and len(token.text) > 2])\n",
    "    lemmatized_texts.append(lemmatized)\n",
    "\n",
    "# Keyword extraction on lemmatized texts\n",
    "vectorizer_lemma = TfidfVectorizer(max_features=10, ngram_range=(1,2))\n",
    "tfidf_matrix_lemma = vectorizer_lemma.fit_transform(lemmatized_texts)\n",
    "keywords_lemma = list(zip(vectorizer_lemma.get_feature_names_out(), tfidf_matrix_lemma.mean(axis=0).A1))\n",
    "keywords_lemma.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"TF-IDF Keywords (Lemmatized Text):\", keywords_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The keywords extracted from lemmatized text are more standardized and may group similar concepts (e.g., \"processing\" and \"process\" both become \"process\"). This reduces redundancy and highlights the most relevant terms. In contrast, keywords from raw text may include multiple forms of the same word, leading to less focused results, also includes a lot of stop words like `and`&`are`. Lemmatization generally improves the quality and interpretability of keyword extraction for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Basic Sentiment Analysis\n",
    "\n",
    "**Sentiment analysis** determines whether text expresses positive, negative, or neutral emotions using word lists.\n",
    "\n",
    "- Useful for: analyzing public opinion, customer feedback, or political discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis: {'sentiment': 'positive', 'positive_words': ['love'], 'negative_words': [], 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "sentiment_result = analyze_sentiment_basic(\"I love Natural Language Processing, but sometimes it is challenging.\")\n",
    "print(\"Sentiment Analysis:\", sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The sentiment score and lists of positive/negative words indicate the overall emotional tone of the text, which can be used to gauge public opinion or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Statistics\n",
    "\n",
    "**Text statistics** provide quantitative measures of text complexity and structure, such as word count, sentence count, and lexical diversity.\n",
    "\n",
    "- Useful for: comparing documents, analyzing readability, and studying vocabulary richness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Statistics: {'word_count': 11, 'sentence_count': 1, 'character_count': 83, 'avg_words_per_sentence': 11.0, 'avg_characters_per_word': 6.55, 'unique_words': 11, 'lexical_diversity': 1.0, 'pos_distribution': {'ADJ': 18.181818181818183, 'NOUN': 36.36363636363637, 'VERB': 27.27272727272727, 'ADP': 9.090909090909092, 'CCONJ': 9.090909090909092}}\n"
     ]
    }
   ],
   "source": [
    "stats = get_text_statistics(text)\n",
    "print(\"Text Statistics:\", stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The statistics provide a quantitative overview of the text, including word and sentence counts, average lengths, vocabulary richness, and part-of-speech distribution. These metrics are useful for comparing documents, assessing complexity, and understanding linguistic characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparing Vocabulary Between Texts\n",
    "\n",
    "**Vocabulary comparison** helps identify similarities and differences in word usage between two texts.\n",
    "\n",
    "- Useful for: comparing speeches, analyzing language differences between groups, or studying terminology evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Comparison: {'common_words': {}, 'unique_to_text1': {'language': 2, 'human': 1, 'understand': 1, 'processing': 1, 'computer': 1, 'enable': 1, 'natural': 1}, 'unique_to_text2': {'learning': 2, 'deep': 1, 'machine': 1, 'artificial': 1, 'important': 1, 'intelligence': 1}, 'similarity_score': 0.0}\n",
      "Vocabulary Comparison: {'common_words': {'human': {'text1_freq': 1, 'text2_freq': 1}, 'text': {'text1_freq': 1, 'text2_freq': 1}, 'processing': {'text1_freq': 1, 'text2_freq': 1}, 'natural': {'text1_freq': 1, 'text2_freq': 1}, 'language': {'text1_freq': 2, 'text2_freq': 2}}, 'unique_to_text1': {'help': 1, 'understand': 1, 'analyze': 1, 'datum': 1, 'computer': 1, 'efficiently': 1}, 'unique_to_text2': {'machine': 1, 'quickly': 1, 'comprehend': 1, 'process': 1, 'enable': 1, 'analytic': 1}, 'similarity_score': 29.41}\n"
     ]
    }
   ],
   "source": [
    "# Two different texts for vocabulary comparison\n",
    "text1 = \"Natural Language Processing enables computers to understand human language.\"\n",
    "text2 = \"Machine learning and deep learning are important for artificial intelligence.\"\n",
    "\n",
    "comparison = compare_texts_vocabulary(text1, text2)\n",
    "print(\"Vocabulary Comparison:\", comparison)\n",
    "\n",
    "\n",
    "# Two similar texts for vocabulary comparison\n",
    "text1 = \"Natural Language Processing helps computers understand human language and analyze text data efficiently.\"\n",
    "text2 = \"Text analytics and Natural Language Processing enable machines to process and comprehend human language quickly.\"\n",
    "\n",
    "# Compare vocabulary usage between the two texts\n",
    "comparison_result = compare_texts_vocabulary(text1, text2)\n",
    "print(\"Vocabulary Comparison:\", comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**  \n",
    "The comparison highlights common and unique words between two texts, helping us understand similarities and differences in language use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored a comprehensive set of text pre-processing techniques using Python, spaCy, and scikit-learn. Starting from basic tokenization and stopword removal, we progressed through lemmatization, sentence segmentation, named entity recognition, and keyword extraction with TF-IDF. We also covered sentiment analysis, text statistics, and vocabulary comparison between texts.\n",
    "\n",
    "These foundational steps are essential for preparing and analyzing textual data in any Natural Language Processing (NLP) project. By mastering these techniques, you can unlock deeper insights from your data, improve the performance of downstream models, and make your analyses more robust and interpretable.\n",
    "\n",
    "Feel free to experiment further with your own texts and datasets. Text pre-processing is a powerful toolâ€”use it to make your NLP workflows more effective and"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
