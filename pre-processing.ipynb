{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing Techniques with spaCy and scikit-learn\n",
    "\n",
    "This notebook demonstrates essential text pre-processing techniques for Natural Language Processing (NLP) using Python. We use spaCy for linguistic analysis and scikit-learn for keyword extraction. Each section introduces a concept, explains its importance, and provides code to apply it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Science Use Cases for Text Pre-Processing\n",
    "\n",
    "Text pre-processing is a crucial step in social science research, enabling scholars to analyze large volumes of qualitative data efficiently and accurately. Here are some practical applications:\n",
    "\n",
    "- **Survey and Interview Analysis:**  \n",
    "  Automatically extract key themes, sentiments, and entities from open-ended survey responses or interview transcripts. For example, lemmatization and stopword removal help in identifying the most frequent topics discussed by participants.\n",
    "\n",
    "- **Political Discourse Analysis:**  \n",
    "  Tokenization, named entity recognition, and sentiment analysis can be used to study political speeches, debates, or social media posts. Researchers can track how politicians discuss certain issues, measure emotional tone, and identify key actors or organizations.\n",
    "\n",
    "- **Media and News Studies:**  \n",
    "  Use sentence segmentation and TF-IDF keyword extraction to compare coverage of events across different news outlets. Named entity recognition helps in mapping relationships between people, places, and organizations mentioned in articles.\n",
    "\n",
    "- **Comparative Linguistic Studies:**  \n",
    "  Vocabulary comparison functions allow researchers to analyze language differences between demographic groups, regions, or time periods. This is useful for studying language evolution, cultural trends, or the impact of policy changes.\n",
    "\n",
    "- **Public Opinion and Sentiment Tracking:**  \n",
    "  Sentiment analysis provides insights into public attitudes toward policies, social issues, or brands by analyzing social media, forums, or feedback forms.\n",
    "\n",
    "By applying these techniques, social scientists can transform unstructured text into actionable data, uncover hidden patterns, and support evidence-based decision-making in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Installing & Importing Libraries\n",
    "\n",
    "Let's start by installing and importing the necessary libraries for text processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.13-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.11-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.10-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/uni/lib/python3.13/site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/uni/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/envs/uni/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/uni/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/uni/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/uni/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached spacy-3.8.7-cp313-cp313-macosx_11_0_arm64.whl (5.9 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp313-cp313-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.13-cp313-cp313-macosx_11_0_arm64.whl (26 kB)\n",
      "Using cached preshed-3.0.10-cp313-cp313-macosx_11_0_arm64.whl (124 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl (632 kB)\n",
      "Using cached thinc-8.3.6-cp313-cp313-macosx_11_0_arm64.whl (832 kB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached blis-1.3.0-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Using cached smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Downloading matplotlib-3.10.5-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached marisa_trie-1.2.1-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-inspection, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, shellingham, pyparsing, pydantic-core, pillow, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, kiwisolver, joblib, idna, fonttools, cycler, cloudpathlib, click, charset_normalizer, certifi, catalogue, annotated-types, srsly, smart-open, scipy, requests, pydantic, preshed, markdown-it-py, language-data, jinja2, contourpy, blis, scikit-learn, rich, matplotlib, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49/49\u001b[0m [spacy]m [spacy]m [thinc]tlib]n]]izer]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.8.3 charset_normalizer-3.4.2 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 contourpy-1.3.3 cycler-0.12.1 cymem-2.0.11 fonttools-4.59.0 idna-3.10 jinja2-3.1.6 joblib-1.5.1 kiwisolver-1.4.8 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 matplotlib-3.10.5 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.3.2 pillow-11.3.0 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 pyparsing-3.2.3 requests-2.32.4 rich-14.1.0 scikit-learn-1.7.1 scipy-1.16.1 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 threadpoolctl-3.6.0 tqdm-4.67.1 typer-0.16.0 typing-inspection-0.4.1 urllib3-2.5.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading spaCy Language Models\n",
    "\n",
    "To process text in different languages, we need to load the appropriate spaCy model. The following functions help load and manage language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'en_core_web_sm' not found. Downloading...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "def get_model(language):\n",
    "    \"\"\"\n",
    "    Loads a spaCy language model for the specified language. If the model is not found,\n",
    "    attempts to download it and then load it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spacy.load(f\"{language}_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(f\"Model '{language}_core_web_sm' not found. Downloading...\")\n",
    "        try:\n",
    "            download_command = f\"python -m spacy download {language}_core_web_sm\"\n",
    "            exit_code = os.system(download_command)\n",
    "        except:\n",
    "            raise ValueError(f\"Language '{language}' is not supported.\")\n",
    "        return spacy.load(f\"{language}_core_web_sm\")\n",
    "\n",
    "def choose_spacy_model(language):\n",
    "    \"\"\"\n",
    "    Choose the appropriate spaCy language model based on the input language.\n",
    "    \"\"\"\n",
    "    return get_model(language)\n",
    "\n",
    "nlp = choose_spacy_model(\"en\")  # Default to English model, can be changed as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "**Tokenization** is the process of splitting text into individual words or tokens. This is the first step in most NLP pipelines.\n",
    "\n",
    "- Useful for: word frequency analysis, further linguistic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', 'with', 'most', 'accuracy', '.', 'it', 'also', 'allows', 'for', 'analyzing', 'text', 'data', 'more', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing enables computers to understand human language with most accuracy. it also allows for analyzing text data more effectively.\"\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize the input text using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_text(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing Stopwords\n",
    "\n",
    "**Stopwords** are common words (like \"the\", \"is\", \"and\") that usually do not add significant meaning to text analysis. Removing them helps focus on meaningful words.\n",
    "\n",
    "- Useful for: keyword extraction, topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stopwords: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'understand', 'human', 'language', 'accuracy', '.', 'allows', 'analyzing', 'text', 'data', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the input text using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens_without_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "print(\"Tokens without stopwords:\", remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatization\n",
    "\n",
    "**Lemmatization** reduces words to their base or dictionary form (lemma). For example, \"running\" becomes \"run\".\n",
    "\n",
    "- Useful for: reducing vocabulary size, improving matching in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized tokens: ['Natural', 'Language', 'processing', 'enable', 'computer', 'to', 'understand', 'human', 'language', 'with', 'most', 'accuracy', '.', 'it', 'also', 'allow', 'for', 'analyze', 'text', 'datum', 'more', 'effectively', '.']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize the input text using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens\n",
    "lemmatized_tokens = lemmatize_text(text)\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentence Segmentation\n",
    "\n",
    "**Sentence segmentation** splits text into individual sentences. This is useful for analyzing sentence structure and readability.\n",
    "\n",
    "- Useful for: readability analysis, sentiment per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Natural Language Processing enables computers to understand human language with most accuracy.', 'it also allows for analyzing text data more effectively.']\n"
     ]
    }
   ],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Split the input text into its sentences using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    splitted_sentences = [sentence.text for sentence in doc.sents]\n",
    "    return splitted_sentences\n",
    "sentences = split_sentences(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Named Entity Recognition (NER)\n",
    "\n",
    "**Named Entity Recognition** identifies and classifies key entities in text, such as people, organizations, and locations.\n",
    "\n",
    "- Useful for: extracting actors, places, and organizations from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [{'text': 'los angeles', 'label': 'GPE', 'description': 'Countries, cities, states'}]\n"
     ]
    }
   ],
   "source": [
    "text_NER = 'the film starring leonardo dicaprio was shot in los angeles and many other locations.'\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"\n",
    "    Extract named entities (people, organizations, locations, etc.) from text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'description': spacy.explain(ent.label_)\n",
    "        })\n",
    "    return entities\n",
    "entities = extract_named_entities(text_NER)\n",
    "print(\"Named Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Keyword Extraction with TF-IDF\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** highlights important words and phrases in a collection of documents.\n",
    "\n",
    "- Useful for: finding distinctive themes, comparing language use across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Keywords: [('learning', np.float64(0.2222222222222222)), ('language', np.float64(0.20100756305184242)), ('analytic', np.float64(0.1360827634879543)), ('large', np.float64(0.1360827634879543)), ('large volume', np.float64(0.1360827634879543)), ('text', np.float64(0.1360827634879543)), ('text analytic', np.float64(0.1360827634879543)), ('volume', np.float64(0.1360827634879543)), ('intelligence', np.float64(0.1111111111111111)), ('learning deep', np.float64(0.1111111111111111)), ('learning important', np.float64(0.1111111111111111)), ('machine', np.float64(0.1111111111111111)), ('machine learning', np.float64(0.1111111111111111)), ('language processing', np.float64(0.10050378152592121)), ('natural', np.float64(0.10050378152592121)), ('natural language', np.float64(0.10050378152592121)), ('processing', np.float64(0.10050378152592121)), ('processing enable', np.float64(0.10050378152592121)), ('understand', np.float64(0.10050378152592121)), ('understand human', np.float64(0.10050378152592121))]\n"
     ]
    }
   ],
   "source": [
    "def extract_keywords_tfidf(texts, max_features=20, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Extract the most important keywords from a collection of texts using TF-IDF analysis.\n",
    "    \"\"\"\n",
    "    if len(texts) < 2:\n",
    "        raise ValueError(\"TF-IDF analysis requires at least 2 documents for comparison.\")\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        processed_text = ' '.join([token.lemma_.lower() for token in doc\n",
    "                                   if not token.is_stop and not token.is_punct\n",
    "                                   and len(token.text) > 2])\n",
    "        processed_texts.append(processed_text)\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    mean_scores = tfidf_matrix.mean(axis=0).A1\n",
    "    keywords_scores = list(zip(feature_names, mean_scores))\n",
    "    keywords_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return keywords_scores\n",
    "\n",
    "texts = [\n",
    "    \"Natural Language Processing enables computers to understand human language.\",\n",
    "    \"Machine learning and deep learning are important for artificial intelligence.\",\n",
    "    \"Text analytics helps in extracting insights from large volumes of data.\"\n",
    "]\n",
    "\n",
    "keywords = extract_keywords_tfidf(texts)\n",
    "print(\"TF-IDF Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Basic Sentiment Analysis\n",
    "\n",
    "**Sentiment analysis** determines whether text expresses positive, negative, or neutral emotions using word lists.\n",
    "\n",
    "- Useful for: analyzing public opinion, customer feedback, or political discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis: {'sentiment': 'positive', 'positive_words': ['love'], 'negative_words': [], 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment_basic(text):\n",
    "    \"\"\"\n",
    "    Perform basic sentiment analysis to determine if text expresses positive or negative emotions.\n",
    "    \"\"\"\n",
    "    positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',\n",
    "                      'love', 'like', 'enjoy', 'happy', 'pleased', 'satisfied', 'positive',\n",
    "                      'benefit', 'advantage', 'success', 'improve', 'better', 'best'}\n",
    "    negative_words = {'bad', 'terrible', 'awful', 'horrible', 'hate', 'dislike', 'angry',\n",
    "                      'sad', 'disappointed', 'frustrated', 'negative', 'problem', 'issue',\n",
    "                      'difficult', 'hard', 'impossible', 'fail', 'failure', 'worse', 'worst'}\n",
    "    doc = nlp(text.lower())\n",
    "    found_positive = []\n",
    "    found_negative = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ in positive_words:\n",
    "            found_positive.append(token.text)\n",
    "        elif token.lemma_ in negative_words:\n",
    "            found_negative.append(token.text)\n",
    "    score = len(found_positive) - len(found_negative)\n",
    "    if score > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif score < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'positive_words': found_positive,\n",
    "        'negative_words': found_negative,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "sentiment_result = analyze_sentiment_basic(\"I love Natural Language Processing, but sometimes it is challenging.\")\n",
    "print(\"Sentiment Analysis:\", sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Statistics\n",
    "\n",
    "**Text statistics** provide quantitative measures of text complexity and structure, such as word count, sentence count, and lexical diversity.\n",
    "\n",
    "- Useful for: comparing documents, analyzing readability, and studying vocabulary richness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Statistics: {'word_count': 14, 'sentence_count': 1, 'character_count': 85, 'avg_words_per_sentence': 14.0, 'avg_characters_per_word': 5.07, 'unique_words': 14, 'lexical_diversity': 1.0, 'pos_distribution': {'DET': 7.142857142857142, 'NOUN': 21.428571428571427, 'VERB': 14.285714285714285, 'PROPN': 21.428571428571427, 'AUX': 7.142857142857142, 'ADP': 7.142857142857142, 'CCONJ': 7.142857142857142, 'ADJ': 14.285714285714285}}\n"
     ]
    }
   ],
   "source": [
    "def get_text_statistics(text):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics about a text document.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    words = [token for token in doc if not token.is_space and not token.is_punct]\n",
    "    sentences = list(doc.sents)\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    character_count = len(text)\n",
    "    avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    word_lengths = [len(token.text) for token in words]\n",
    "    avg_characters_per_word = sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
    "    word_texts = [token.text.lower() for token in words if token.is_alpha]\n",
    "    unique_words = len(set(word_texts))\n",
    "    lexical_diversity = unique_words / len(word_texts) if word_texts else 0\n",
    "    pos_counts = {}\n",
    "    for token in words:\n",
    "        pos = token.pos_\n",
    "        pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "    pos_distribution = {pos: (count/word_count)*100 for pos, count in pos_counts.items()}\n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'character_count': character_count,\n",
    "        'avg_words_per_sentence': round(avg_words_per_sentence, 2),\n",
    "        'avg_characters_per_word': round(avg_characters_per_word, 2),\n",
    "        'unique_words': unique_words,\n",
    "        'lexical_diversity': round(lexical_diversity, 3),\n",
    "        'pos_distribution': pos_distribution\n",
    "    }\n",
    "stats = get_text_statistics(text)\n",
    "print(\"Text Statistics:\", stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparing Vocabulary Between Texts\n",
    "\n",
    "**Vocabulary comparison** helps identify similarities and differences in word usage between two texts.\n",
    "\n",
    "- Useful for: comparing speeches, analyzing language differences between groups, or studying terminology evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Comparison: {'common_words': {}, 'unique_to_text1': {'language': 2, 'natural': 1, 'human': 1, 'computer': 1, 'enable': 1, 'understand': 1, 'processing': 1}, 'unique_to_text2': {'learning': 2, 'artificial': 1, 'deep': 1, 'machine': 1, 'important': 1, 'intelligence': 1}, 'similarity_score': 0.0}\n",
      "Vocabulary Comparison: {'common_words': {'natural': {'text1_freq': 1, 'text2_freq': 1}, 'human': {'text1_freq': 1, 'text2_freq': 1}, 'language': {'text1_freq': 2, 'text2_freq': 2}, 'text': {'text1_freq': 1, 'text2_freq': 1}, 'processing': {'text1_freq': 1, 'text2_freq': 1}}, 'unique_to_text1': {'computer': 1, 'datum': 1, 'efficiently': 1, 'understand': 1, 'analyze': 1, 'help': 1}, 'unique_to_text2': {'enable': 1, 'process': 1, 'analytic': 1, 'machine': 1, 'quickly': 1, 'comprehend': 1}, 'similarity_score': 29.41}\n"
     ]
    }
   ],
   "source": [
    "def compare_texts_vocabulary(text1, text2, top_n=10):\n",
    "    \"\"\"\n",
    "    Compare the vocabulary usage between two texts to identify similarities and differences.\n",
    "    \"\"\"\n",
    "    doc1 = nlp(text1.lower())\n",
    "    doc2 = nlp(text2.lower())\n",
    "    words1 = [token.lemma_ for token in doc1 if not token.is_stop and not token.is_punct\n",
    "              and token.is_alpha and len(token.text) > 2]\n",
    "    words2 = [token.lemma_ for token in doc2 if not token.is_stop and not token.is_punct\n",
    "              and token.is_alpha and len(token.text) > 2]\n",
    "    freq1 = Counter(words1)\n",
    "    freq2 = Counter(words2)\n",
    "    set1 = set(freq1.keys())\n",
    "    set2 = set(freq2.keys())\n",
    "    common_words = {}\n",
    "    for word in set1.intersection(set2):\n",
    "        common_words[word] = {'text1_freq': freq1[word], 'text2_freq': freq2[word]}\n",
    "    unique_to_text1 = {word: freq1[word] for word in set1 - set2}\n",
    "    unique_to_text2 = {word: freq2[word] for word in set2 - set1}\n",
    "    unique_to_text1 = dict(sorted(unique_to_text1.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "    unique_to_text2 = dict(sorted(unique_to_text2.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "    total_unique_words = len(set1.union(set2))\n",
    "    similarity_score = (len(common_words) / total_unique_words * 100) if total_unique_words > 0 else 0\n",
    "    return {\n",
    "        'common_words': common_words,\n",
    "        'unique_to_text1': unique_to_text1,\n",
    "        'unique_to_text2': unique_to_text2,\n",
    "        'similarity_score': round(similarity_score, 2)\n",
    "    }\n",
    "\n",
    "# Two different texts for vocabulary comparison\n",
    "text1 = \"Natural Language Processing enables computers to understand human language.\"\n",
    "text2 = \"Machine learning and deep learning are important for artificial intelligence.\"\n",
    "\n",
    "comparison = compare_texts_vocabulary(text1, text2)\n",
    "print(\"Vocabulary Comparison:\", comparison)\n",
    "\n",
    "\n",
    "# Two similar texts for vocabulary comparison\n",
    "text1 = \"Natural Language Processing helps computers understand human language and analyze text data efficiently.\"\n",
    "text2 = \"Text analytics and Natural Language Processing enable machines to process and comprehend human language quickly.\"\n",
    "\n",
    "# Compare vocabulary usage between the two texts\n",
    "comparison_result = compare_texts_vocabulary(text1, text2)\n",
    "print(\"Vocabulary Comparison:\", comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored a comprehensive set of text pre-processing techniques using Python, spaCy, and scikit-learn. Starting from basic tokenization and stopword removal, we progressed through lemmatization, sentence segmentation, named entity recognition, and keyword extraction with TF-IDF. We also covered sentiment analysis, text statistics, and vocabulary comparison between texts.\n",
    "\n",
    "These foundational steps are essential for preparing and analyzing textual data in any Natural Language Processing (NLP) project. By mastering these techniques, you can unlock deeper insights from your data, improve the performance of downstream models, and make your analyses more robust and interpretable.\n",
    "\n",
    "Feel free to experiment further with your own texts and datasets. Text pre-processing is a powerful tool—use it to make your NLP workflows more effective and"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
